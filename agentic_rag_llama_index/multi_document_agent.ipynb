{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity scores. Additionally, the models are fine-tuned on different context lengths, such as 8192, 32768, 100k, 65536, and 32768, with promising results on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity scores. The models are fine-tuned on different context lengths, such as 8192, 32768, 100k, 65536, and 32768, with promising results on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It trains a single LM to retrieve, generate, and critique text passages using special tokens called reflection tokens. This framework significantly outperforms other LLMs and retrieval-augmented models on various tasks, demonstrating improved performance in terms of factuality, correctness, and fluency. Additionally, Self-RAG evaluates the factual precision and usefulness of generated text by analyzing relevance, evidence support, and helpfulness in answering queries. It aims to ensure that the generated text is factually accurate, well-supported, and informative in response to various queries or instructions.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise. It introduces Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training, allowing for significantly extending the context window of LLMs while reducing GPU memory cost and training time compared to standard full fine-tuning. Additionally, LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure. Furthermore, LongLoRA includes the Action Units Relation Transformer (ART) for modeling relations between facial action units and the Tampered AU Prediction (TAP) for tampering AU-related regions to improve model generalization, achieving state-of-the-art performance on cross-dataset and cross-manipulation evaluations.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It trains a single LM to retrieve, generate, and critique text passages using special tokens called reflection tokens. This framework significantly outperforms other LLMs and retrieval-augmented models on various tasks, demonstrating improved performance in terms of factuality, correctness, and fluency. Additionally, Self-RAG evaluates the factual precision and usefulness of generated text by analyzing relevance, evidence support, and helpfulness in answering queries. It aims to ensure that the generated text is factually accurate, well-supported, and informative in response to various queries or instructions.\n",
      "\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise. It introduces Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training, allowing for significantly extending the context window of LLMs while reducing GPU memory cost and training time compared to standard full fine-tuning. Additionally, LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure. Furthermore, LongLoRA includes the Action Units Relation Transformer (ART) for modeling relations between facial action units and the Tampered AU Prediction (TAP) for tampering AU-related regions to improve model generalization, achieving state-of-the-art performance on cross-dataset and cross-manipulation evaluations.\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It trains a single LM to retrieve, generate, and critique text passages using special tokens called reflection tokens. This framework significantly outperforms other LLMs and retrieval-augmented models on various tasks, demonstrating improved performance in terms of factuality, correctness, and fluency. Additionally, Self-RAG evaluates the factual precision and usefulness of generated text by analyzing relevance, evidence support, and helpfulness in answering queries. It aims to ensure that the generated text is factually accurate, well-supported, and informative in response to various queries or instructions.\n",
      "\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise. It introduces Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training, allowing for significantly extending the context window of LLMs while reducing GPU memory cost and training time compared to standard full fine-tuning. Additionally, LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure. Furthermore, LongLoRA includes the Action Units Relation Transformer (ART) for modeling relations between facial action units and the Tampered AU Prediction (TAP) for tampering AU-related regions to improve model generalization, achieving state-of-the-art performance on cross-dataset and cross-manipulation evaluations.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    # \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When you have many tools\n",
    "\n",
    "When you're indexing so many tools:\n",
    "* Tools may not all fit in the prompt\n",
    "* Cost and latency spikes due to increase in number of tokens.\n",
    "* LLM can pick the wrong tool when the number of choices is too large.\n",
    "\n",
    "Solution:\n",
    "* When the user asks a query, perform retrieval augmentation on the tools.\n",
    "* First retrieve a small set of relevant tools and feed it to the agent reasoning prompt instead of all tools.\n",
    "* Similar to retrieval process in RAG.\n",
    "* Can be top k vector search, but can add advanced filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes the HumanEval benchmark, the MBPP benchmark, and the SoftwareDev dataset. The HumanEval benchmark consists of 164 handwritten programming tasks, the MBPP benchmark comprises 427 Python tasks, and the SoftwareDev dataset includes 70 representative examples of software development tasks covering diverse scopes such as mini-games, image processing algorithms, and data visualization.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench is constructed by scraping pull requests from the top 100 most downloaded Python libraries on PyPI. It consists of task instances extracted from various open-source repositories, with each task instance including a codebase snapshot, a description of the issue to be resolved, and the associated pull request changes. The dataset is continuously updated to include new task instances based on PRs created after the training date of language models. Task instances are validated through execution-based testing to ensure correctness and non-triviality of the solutions. The dataset is designed to be easily extensible to new programming languages and code domains, allowing for continual updates and temporal robustness.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes the HumanEval benchmark, the MBPP benchmark, and the SoftwareDev dataset. The HumanEval benchmark consists of 164 handwritten programming tasks, the MBPP benchmark comprises 427 Python tasks, and the SoftwareDev dataset includes 70 representative examples of software development tasks covering diverse scopes such as mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench is constructed by scraping pull requests from the top 100 most downloaded Python libraries on PyPI. It consists of task instances extracted from various open-source repositories, with each task instance including a codebase snapshot, a description of the issue to be resolved, and the associated pull request changes. The dataset is continuously updated to include new task instances based on PRs created after the training date of language models. Task instances are validated through execution-based testing to ensure correctness and non-triviality of the solutions. The dataset is designed to be easily extensible to new programming languages and code domains, allowing for continual updates and temporal robustness.\n",
      "The evaluation dataset used in MetaGPT includes the HumanEval benchmark, the MBPP benchmark, and the SoftwareDev dataset. The HumanEval benchmark consists of 164 handwritten programming tasks, the MBPP benchmark comprises 427 Python tasks, and the SoftwareDev dataset includes 70 representative examples of software development tasks covering diverse scopes such as mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench is constructed by scraping pull requests from the top 100 most downloaded Python libraries on PyPI. It consists of task instances extracted from various open-source repositories, with each task instance including a codebase snapshot, a description of the issue to be resolved, and the associated pull request changes. The dataset is continuously updated to include new task instances based on PRs created after the training date of language models. Task instances are validated through execution-based testing to ensure correctness and non-triviality of the solutions. The dataset is designed to be easily extensible to new programming languages and code domains, allowing for continual updates and temporal robustness.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Analyzing the approach in the LongLoRA paper.\"}\n",
      "=== Function Output ===\n",
      "The approach introduced in the LongLoRA paper focuses on extending the context length of large language models efficiently by combining the LoRA method with shifted sparse attention during training. This method splits the context length into groups and conducts attention within each group individually, allowing for information flow between neighboring groups. The paper also highlights the significance of trainable normalization and embedding layers for successful adaptation to longer contexts. Additionally, the LongLoRA paper introduces the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components for forgery detection in deepfake videos. The ART models relations between facial action units at AU-agnostic patches, while the TAP process tampers with AU-related regions to enhance the model's generalization to unseen manipulation methods. The approach in the LongLoRA paper achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, provides visualizations of tampered regions, and offers an effective framework for deepfake detection.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Analyzing the approach in the LoftQ paper.\"}\n",
      "=== Function Output ===\n",
      "The LoftQ paper presents a novel quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This method aims to provide a suitable initialization for subsequent LoRA fine-tuning, narrowing the gap between quantized weights and full-precision weights. LoftQ has demonstrated superior performance compared to existing quantization methods, particularly in low-bit quantization scenarios. It has been effective across various tasks like natural language understanding, question answering, summarization, and natural language generation. Additionally, LoftQ is computationally light and has low memory requirements, making it a promising approach for model compression and efficient downstream task adaptation. The approach targets model compression, specifically focusing on the DeBERTaV3-base model, aiming to achieve performance close to full fine-tuning while reducing memory requirements during training and storage. By incorporating low-rank adapters, LoftQ can compress neural network models efficiently, including convolutional layers. The method involves hyperparameter tuning to optimize the quantization process for different tasks, showcasing promising results compared to state-of-the-art pruning methods and effectively compressing models without significant performance loss.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper focuses on extending the context length of large language models efficiently by combining the LoRA method with shifted sparse attention during training. It splits the context length into groups and conducts attention within each group individually, allowing for information flow between neighboring groups. The paper also introduces the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components for forgery detection in deepfake videos. The ART models relations between facial action units at AU-agnostic patches, while the TAP process tampers with AU-related regions to enhance the model's generalization to unseen manipulation methods. The LongLoRA paper achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, provides visualizations of tampered regions, and offers an effective framework for deepfake detection.\n",
      "\n",
      "On the other hand, the LoftQ paper presents a quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This framework aims to provide a suitable initialization for subsequent LoRA fine-tuning, narrowing the gap between quantized weights and full-precision weights. LoftQ has demonstrated superior performance compared to existing quantization methods, particularly in low-bit quantization scenarios. It is computationally light, has low memory requirements, and is effective across various tasks like natural language understanding, question answering, summarization, and natural language generation. LoftQ targets model compression, specifically focusing on the DeBERTaV3-base model, aiming to achieve performance close to full fine-tuning while reducing memory requirements during training and storage. The method involves hyperparameter tuning to optimize the quantization process for different tasks, showcasing promising results compared to state-of-the-art pruning methods and effectively compressing models without significant performance loss.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
